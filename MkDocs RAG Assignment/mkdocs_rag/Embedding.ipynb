{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "       PYTORCH GPU DIAGNOSTIC FOR USER       \n",
            "--------------------------------------------------\n",
            "âœ… CUDA is available! (PyTorch Version: 2.6.0+cu124)\n",
            "ğŸ’» GPU Detected:   NVIDIA GeForce MX150\n",
            "ğŸ”¢ CUDA Version:   12.4\n",
            "\n",
            "... Attempting actual computation on GPU ...\n",
            "âœ… Success! performed matrix multiplication on NVIDIA GeForce MX150.\n",
            "   Result Tensor location: cuda:0\n",
            "   Output shape: torch.Size([5, 5])\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def test_gpu():\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"       PYTORCH GPU DIAGNOSTIC FOR USER       \")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    # 1. Check if CUDA is available\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"âœ… CUDA is available! (PyTorch Version: {torch.__version__})\")\n",
        "        \n",
        "        # 2. Get Device Details\n",
        "        device_id = torch.cuda.current_device()\n",
        "        gpu_name = torch.cuda.get_device_name(device_id)\n",
        "        print(f\"ğŸ’» GPU Detected:   {gpu_name}\")\n",
        "        print(f\"ğŸ”¢ CUDA Version:   {torch.version.cuda}\")\n",
        "        \n",
        "        # 3. Perform a Real Calculation on GPU\n",
        "        # We create two random tensors and multiply them on the VRAM\n",
        "        try:\n",
        "            print(\"\\n... Attempting actual computation on GPU ...\")\n",
        "            x = torch.rand(5, 3).cuda()\n",
        "            y = torch.rand(3, 5).cuda()\n",
        "            result = torch.matmul(x, y)\n",
        "            \n",
        "            print(f\"âœ… Success! performed matrix multiplication on {gpu_name}.\")\n",
        "            print(f\"   Result Tensor location: {result.device}\")\n",
        "            print(\"   Output shape:\", result.shape)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error during computation: {e}\")\n",
        "            \n",
        "    else:\n",
        "        print(\"âŒ CUDA is NOT available.\")\n",
        "        print(\"   PyTorch is running on CPU only.\")\n",
        "\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_gpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… GPU Available: NVIDIA GeForce MX150\n",
            "   CUDA Version: 12.4\n",
            "   GPU Memory: 4.29 GB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Environment loaded\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Setup and Environment (GPU Optimized)\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Check for GPU availability\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "        DEVICE = \"cuda\"\n",
        "    else:\n",
        "        print(\"â„¹ï¸ GPU not available, using CPU\")\n",
        "        DEVICE = \"cpu\"\n",
        "except ImportError:\n",
        "    print(\"â„¹ï¸ PyTorch not installed, using CPU\")\n",
        "    DEVICE = \"cpu\"\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "print(\"âœ… Environment loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.13) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ChromaDB and Google AI clients initialized\n",
            "   Embedding Model: Google text-embedding-005\n",
            "   Vector DB: ChromaDB (PersistentClient)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 2: Initialize ChromaDB and Google AI\n",
        "from google import genai\n",
        "import chromadb.utils.embedding_functions as embedding_functions\n",
        "import chromadb\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"db/\")\n",
        "google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(\n",
        "    api_key=api_key,\n",
        "    model_name='models/text-embedding-004'\n",
        ")\n",
        "client = genai.Client(api_key=api_key)\n",
        "print(\"âœ… ChromaDB and Google AI clients initialized\")\n",
        "print(\"   Embedding Model: Google text-embedding-005\")\n",
        "print(\"   Vector DB: ChromaDB (PersistentClient)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Collection 'MkDocsRAG' ready (existing count: 360)\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Create/Get Collection\n",
        "collection = chroma_client.get_or_create_collection(name=\"MkDocsRAG\", embedding_function=google_ef)\n",
        "print(f\"âœ… Collection 'MkDocsRAG' ready (existing count: {collection.count()})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load Markdown Files from Local Directory\n",
        "\n",
        "Load markdown documentation files from a local directory.\n",
        "Place your markdown files in the `mkdocs_docs/` directory (or specify a custom path).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Loading markdown files from local directory...\n",
            "ğŸ“‚ Found 19 markdown files\n",
            "âœ… Loaded: getting-started.md\n",
            "âœ… Loaded: index.md\n",
            "âœ… Loaded: about\\contributing.md\n",
            "âœ… Loaded: about\\license.md\n",
            "âœ… Loaded: about\\release-notes.md\n",
            "âœ… Loaded: dev-guide\\api.md\n",
            "âœ… Loaded: dev-guide\\plugins.md\n",
            "âœ… Loaded: dev-guide\\README.md\n",
            "âœ… Loaded: dev-guide\\themes.md\n",
            "âœ… Loaded: dev-guide\\translations.md\n",
            "âœ… Loaded: user-guide\\choosing-your-theme.md\n",
            "âœ… Loaded: user-guide\\cli.md\n",
            "âœ… Loaded: user-guide\\configuration.md\n",
            "âœ… Loaded: user-guide\\customizing-your-theme.md\n",
            "âœ… Loaded: user-guide\\deploying-your-docs.md\n",
            "âœ… Loaded: user-guide\\installation.md\n",
            "âœ… Loaded: user-guide\\localizing-your-theme.md\n",
            "âœ… Loaded: user-guide\\README.md\n",
            "âœ… Loaded: user-guide\\writing-your-docs.md\n",
            "âœ… Loaded 19 documentation files\n",
            "âœ… Metadata saved to extraction_metadata.json\n",
            "âœ… Loading complete! Found 19 documentation files\n",
            "ğŸš€ Loading markdown files from local directory...\n",
            "ğŸ“‚ Found 19 markdown files\n",
            "âœ… Loaded: getting-started.md\n",
            "âœ… Loaded: index.md\n",
            "âœ… Loaded: about\\contributing.md\n",
            "âœ… Loaded: about\\license.md\n",
            "âœ… Loaded: about\\release-notes.md\n",
            "âœ… Loaded: dev-guide\\api.md\n",
            "âœ… Loaded: dev-guide\\plugins.md\n",
            "âœ… Loaded: dev-guide\\README.md\n",
            "âœ… Loaded: dev-guide\\themes.md\n",
            "âœ… Loaded: dev-guide\\translations.md\n",
            "âœ… Loaded: user-guide\\choosing-your-theme.md\n",
            "âœ… Loaded: user-guide\\cli.md\n",
            "âœ… Loaded: user-guide\\configuration.md\n",
            "âœ… Loaded: user-guide\\customizing-your-theme.md\n",
            "âœ… Loaded: user-guide\\deploying-your-docs.md\n",
            "âœ… Loaded: user-guide\\installation.md\n",
            "âœ… Loaded: user-guide\\localizing-your-theme.md\n",
            "âœ… Loaded: user-guide\\README.md\n",
            "âœ… Loaded: user-guide\\writing-your-docs.md\n",
            "âœ… Loaded 19 documentation files\n",
            "âœ… Metadata saved to extraction_metadata.json\n",
            "âœ… Loading complete! Found 19 documentation files\n"
          ]
        }
      ],
      "source": [
        "# Load Markdown Files from Local Directory\n",
        "import re\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "\n",
        "# Configuration - Set your markdown files directory here\n",
        "DOCS_DIR = r\"M:\\Term 9\\Image Processing and Pattern Recognition\\RAG Assignment\\MkDocs RAG Assignment\\mkdocs_rag\\docs\"  # Change this to your markdown files directory\n",
        "\n",
        "\n",
        "\n",
        "def clean_markdown_content(content: str) -> str:\n",
        "    \"\"\"Clean markdown content\"\"\"\n",
        "    content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
        "    lines = [line.rstrip() for line in content.split('\\n')]\n",
        "    content = '\\n'.join(lines)\n",
        "    content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)\n",
        "    content = re.sub(r'^(#{1,6})([^\\s#])', r'\\1 \\2', content, flags=re.MULTILINE)\n",
        "    return content.strip()\n",
        "\n",
        "def load_documentation_files(docs_dir: str = DOCS_DIR) -> List[Dict]:\n",
        "    \"\"\"Load markdown documentation files from local directory\"\"\"\n",
        "    docs_path = Path(docs_dir)\n",
        "    \n",
        "    if not docs_path.exists():\n",
        "        print(f\"âŒ Documentation directory not found at {docs_path}\")\n",
        "        print(f\"ğŸ’¡ Please create the directory and add your markdown files, or update DOCS_DIR variable\")\n",
        "        return []\n",
        "    \n",
        "    documentation_files = []\n",
        "    \n",
        "    # Find all markdown files\n",
        "    md_files = list(docs_path.rglob(\"*.md\"))\n",
        "    \n",
        "    if not md_files:\n",
        "        print(f\"âš ï¸ No markdown files found in {docs_path}\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"ğŸ“‚ Found {len(md_files)} markdown files\")\n",
        "    \n",
        "    for md_file in md_files:\n",
        "        relative_path = md_file.relative_to(docs_path)\n",
        "        \n",
        "        try:\n",
        "            with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read()\n",
        "            \n",
        "            cleaned_content = clean_markdown_content(content)\n",
        "            \n",
        "            documentation_files.append({\n",
        "                'file_path': str(relative_path),\n",
        "                'content': cleaned_content,\n",
        "                'source': str(md_file)\n",
        "            })\n",
        "            \n",
        "            print(f\"âœ… Loaded: {relative_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error loading {relative_path}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"âœ… Loaded {len(documentation_files)} documentation files\")\n",
        "    return documentation_files\n",
        "\n",
        "# Load documentation files\n",
        "print(\"ğŸš€ Loading markdown files from local directory...\")\n",
        "files = load_documentation_files(DOCS_DIR)\n",
        "\n",
        "if files:\n",
        "    metadata = {\n",
        "        'total_files': len(files),\n",
        "        'files': [\n",
        "            {\n",
        "                'file_path': f['file_path'],\n",
        "                'content_length': len(f['content']),\n",
        "                'source': f['source']\n",
        "            }\n",
        "            for f in files\n",
        "        ]\n",
        "    }\n",
        "    with open('extraction_metadata.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"âœ… Metadata saved to extraction_metadata.json\")\n",
        "\n",
        "print(f\"âœ… Loading complete! Found {len(files)} documentation files\")\n",
        "\n",
        "# Load documentation files\n",
        "print(\"ğŸš€ Loading markdown files from local directory...\")\n",
        "files = load_documentation_files(DOCS_DIR)\n",
        "\n",
        "if files:\n",
        "    metadata = {\n",
        "        'total_files': len(files),\n",
        "        'files': [\n",
        "            {\n",
        "                'file_path': f['file_path'],\n",
        "                'content_length': len(f['content']),\n",
        "                'source': f['source']\n",
        "            }\n",
        "            for f in files\n",
        "        ]\n",
        "    }\n",
        "    with open('extraction_metadata.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"âœ… Metadata saved to extraction_metadata.json\")\n",
        "\n",
        "print(f\"âœ… Loading complete! Found {len(files)} documentation files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Chunking Implementation\n",
        "\n",
        "**DELIVERABLE 1 & 2**: Chunking method selection and cleaning implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Chunker initialized\n",
            "Strategy: RecursiveCharacterTextSplitter with markdown-aware separators\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class MkDocsChunker:\n",
        "    \"\"\"\n",
        "    Chunking strategy: RecursiveCharacterTextSplitter\n",
        "    \n",
        "    Reason for selection:\n",
        "    1. MkDocs documentation is primarily markdown with hierarchical structure\n",
        "    2. RecursiveCharacterTextSplitter respects markdown structure (headers, code blocks, lists)\n",
        "    3. Handles variable-length content well (short code snippets to long explanations)\n",
        "    4. Preserves context through chunk overlap\n",
        "    5. Works well with semantic search as it maintains semantic boundaries\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200, separators: List[str] = None):\n",
        "        if separators is None:\n",
        "            separators = [\n",
        "                \"\\n\\n## \",      # Major sections\n",
        "                \"\\n\\n### \",     # Subsections\n",
        "                \"\\n\\n\",         # Paragraph breaks\n",
        "                \"\\n\",           # Line breaks\n",
        "                \". \",           # Sentences\n",
        "                \" \",            # Words\n",
        "                \"\"              # Characters\n",
        "            ]\n",
        "        \n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=separators,\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            is_separator_regex=False\n",
        "        )\n",
        "    \n",
        "    \n",
        "    def clean_chunk(self, chunk: str) -> str:\n",
        "        \"\"\"Clean individual chunk - 5-step cleaning process\"\"\"\n",
        "        # Step 1: Remove leading/trailing whitespace\n",
        "        chunk = chunk.strip()\n",
        "        \n",
        "        # Step 2: Remove excessive blank lines (more than 2 consecutive)\n",
        "        chunk = re.sub(r'\\n{3,}', '\\n\\n', chunk)\n",
        "        \n",
        "        # Step 3: Remove whitespace-only lines at start/end\n",
        "        lines = chunk.split('\\n')\n",
        "        while lines and not lines[0].strip():\n",
        "            lines.pop(0)\n",
        "        while lines and not lines[-1].strip():\n",
        "            lines.pop(-1)\n",
        "        chunk = '\\n'.join(lines)\n",
        "        \n",
        "        # Step 4: Normalize markdown header formatting\n",
        "        chunk = re.sub(r'\\n(#{1,6})\\s*([^\\n]+)', r'\\n\\n\\1 \\2\\n', chunk)\n",
        "        \n",
        "        # Step 5: Final cleanup of excessive newlines\n",
        "        chunk = re.sub(r'\\n{3,}', '\\n\\n', chunk)\n",
        "        \n",
        "        return chunk.strip()\n",
        "    \n",
        "    def chunk_document(self, content: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Chunk a document and return chunks with metadata\"\"\"\n",
        "        chunks = self.splitter.split_text(content)\n",
        "        cleaned_chunks = [self.clean_chunk(chunk) for chunk in chunks]\n",
        "        cleaned_chunks = [chunk for chunk in cleaned_chunks if len(chunk) > 50]\n",
        "        \n",
        "        chunk_list = []\n",
        "        for i, chunk in enumerate(cleaned_chunks):\n",
        "            chunk_metadata = {\n",
        "                'chunk_index': i,\n",
        "                'chunk_size': len(chunk),\n",
        "                'total_chunks': len(cleaned_chunks)\n",
        "            }\n",
        "            if metadata:\n",
        "                chunk_metadata.update(metadata)\n",
        "            \n",
        "            chunk_list.append({\n",
        "                'content': chunk,\n",
        "                'metadata': chunk_metadata\n",
        "            })\n",
        "        \n",
        "        return chunk_list\n",
        "\n",
        "# Initialize chunker\n",
        "chunker = MkDocsChunker(chunk_size=1000, chunk_overlap=200)\n",
        "print(\"âœ… Chunker initialized\")\n",
        "print(\"Strategy: RecursiveCharacterTextSplitter with markdown-aware separators\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Process and Chunk Documentation\n",
        "\n",
        "Load extracted documentation and chunk it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading and chunking docs: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading and chunking docs: 19it [00:00, 309.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 417 chunks from 19 files\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Process and chunk all documentation\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "docs_dir = Path(DOCS_DIR)\n",
        "if not docs_dir.exists():\n",
        "    print(\"âŒ mkdocs_docs directory not found. Run the extraction cell first!\")\n",
        "else:\n",
        "    all_texts = []\n",
        "    all_metadatas = []\n",
        "    all_ids = []\n",
        "    \n",
        "    # Process all markdown files\n",
        "    for md_file in tqdm(docs_dir.rglob(\"*.md\"), desc=\"Loading and chunking docs\"):\n",
        "        with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            content = f.read()\n",
        "        \n",
        "        relative_path = md_file.relative_to(docs_dir)\n",
        "        \n",
        "        # Chunk the document\n",
        "        chunks = chunker.chunk_document(\n",
        "            content,\n",
        "            metadata={'file_path': str(relative_path), 'source': 'mkdocs'}\n",
        "        )\n",
        "        \n",
        "        # Add chunks to lists\n",
        "        for chunk in chunks:\n",
        "            all_texts.append(chunk['content'])\n",
        "            all_metadatas.append(chunk['metadata'])\n",
        "            all_ids.append(f\"{relative_path}-c{chunk['metadata']['chunk_index']}\")\n",
        "    \n",
        "    # Save to file for caching\n",
        "    with open(\"split_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump((all_texts, all_metadatas, all_ids), f)\n",
        "    \n",
        "    print(f\"âœ… Processed {len(all_texts)} chunks from {len(list(docs_dir.rglob('*.md')))} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Embed and Index Chunks\n",
        "\n",
        "Embed chunks and add them to ChromaDB.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded 417 chunks.\n",
            "âœ… Processing 0 to 30 (30 chunks)\n",
            "âœ… Processing 30 to 60 (30 chunks)\n",
            "âœ… Processing 60 to 90 (30 chunks)\n",
            "âœ… Processing 90 to 120 (30 chunks)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding Chunks:   0%|          | 0/14 [00:00<?, ?batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 120 to 150 (30 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 150 to 180 (30 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 180 to 210 (30 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 210 to 240 (30 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 240 to 270 (30 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 270 to 300 (30 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 300 to 330 (30 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 330 to 360 (30 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 360 to 390 (30 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "âœ… Processing 390 to 417 (27 chunks)\n",
            "â© All IDs already exist. Skipping batch.\n",
            "â© All IDs already exist. Skipping batch.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding Chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:19<00:00,  1.36s/batch]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Indexing completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load split data\n",
        "import threading\n",
        "import time\n",
        "\n",
        "with open(\"split_data.pkl\", \"rb\") as f:\n",
        "    all_texts, all_metadatas, all_ids = pickle.load(f)\n",
        "\n",
        "print(f\"âœ… Loaded {len(all_texts)} chunks.\")\n",
        "\n",
        "# Threading configuration\n",
        "NUM_WORKERS = 4\n",
        "BATCH_SIZE = 30\n",
        "total = len(all_texts)\n",
        "\n",
        "def embed_and_insert(start_idx: int, end_idx: int):\n",
        "    texts = all_texts[start_idx:end_idx]\n",
        "    metadatas = all_metadatas[start_idx:end_idx]\n",
        "    ids = all_ids[start_idx:end_idx]\n",
        "    \n",
        "    print(f\"âœ… Processing {start_idx} to {end_idx} ({len(texts)} chunks)\")\n",
        "\n",
        "    # Check which IDs already exist\n",
        "    try:\n",
        "        existing = collection.get(ids=ids)\n",
        "        existing_ids = set(existing[\"ids\"])\n",
        "    except Exception as e:\n",
        "        existing_ids = set()\n",
        "\n",
        "    # Filter out already existing IDs\n",
        "    filtered_texts, filtered_metadatas, filtered_ids = [], [], []\n",
        "    for t, m, i in zip(texts, metadatas, ids):\n",
        "        if i not in existing_ids:\n",
        "            filtered_texts.append(t)\n",
        "            filtered_metadatas.append(m)\n",
        "            filtered_ids.append(i)\n",
        "\n",
        "    if not filtered_ids:\n",
        "        print(\"â© All IDs already exist. Skipping batch.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        collection.add(documents=filtered_texts, metadatas=filtered_metadatas, ids=filtered_ids)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Retry after error: {e}\")\n",
        "        time.sleep(60)\n",
        "        try:\n",
        "            collection.add(documents=filtered_texts, metadatas=filtered_metadatas, ids=filtered_ids)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Permanent failure: {e}\")\n",
        "    \n",
        "    time.sleep(4)\n",
        "\n",
        "# Run embedding\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "total = len(all_texts)\n",
        "futures = []\n",
        "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
        "    for i in range(0, total, BATCH_SIZE):\n",
        "        futures.append(executor.submit(embed_and_insert, i, min(i + BATCH_SIZE, total)))\n",
        "\n",
        "    for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Embedding Chunks\", unit=\"batch\"):\n",
        "        try:\n",
        "            _.result()\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Thread failed: {e}\")\n",
        "\n",
        "print(\"âœ… Indexing completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Verify Collection\n",
        "\n",
        "Check the indexed documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total documents in collection: 417\n",
            "\n",
            "Sample documents:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ids': ['getting-started.md-c0',\n",
              "  'getting-started.md-c1',\n",
              "  'getting-started.md-c2'],\n",
              " 'embeddings': array([[ 0.01560513, -0.03055065, -0.01265503, ..., -0.02278913,\n",
              "          0.05122728,  0.03479302],\n",
              "        [ 0.00279664, -0.02567179, -0.00038613, ..., -0.00740974,\n",
              "          0.03781191,  0.04635596],\n",
              "        [ 0.0372159 , -0.0196034 , -0.01597871, ..., -0.01985432,\n",
              "          0.02966963,  0.03374023]]),\n",
              " 'documents': ['# Getting Started with MkDocs\\n\\nAn introductory tutorial!\\n\\n---\\n\\n## Installation\\n\\nTo install MkDocs, run the following command from the command line:\\n\\n```bash\\npip install mkdocs\\n```\\n\\nFor more details, see the [Installation Guide].',\n",
              "  \"## Creating a new project\\n\\nGetting started is super easy. To create a new project, run the following\\ncommand from the command line:\\n\\n```bash\\nmkdocs new my-project\\ncd my-project\\n```\\n\\nTake a moment to review the initial project that has been created for you.\\n\\n![The initial MkDocs layout](img/initial-layout.png)\\n\\nThere's a single configuration file named `mkdocs.yml`, and a folder named\\n`docs` that will contain your documentation source files (`docs` is\\nthe default value for the [docs_dir] configuration setting). Right now the `docs`\\nfolder just contains a single documentation page, named `index.md`.\\n\\nMkDocs comes with a built-in dev-server that lets you preview your documentation\\nas you work on it. Make sure you're in the same directory as the `mkdocs.yml`\\nconfiguration file, and then start the server by running the `mkdocs serve`\\ncommand:\",\n",
              "  \"```console\\n$ mkdocs serve\\nINFO    -  Building documentation...\\nINFO    -  Cleaning site directory\\nINFO    -  Documentation built in 0.22 seconds\\nINFO    -  [15:50:43] Watching paths for changes: 'docs', 'mkdocs.yml'\\nINFO    -  [15:50:43] Serving on http://127.0.0.1:8000/\\n```\\n\\nOpen up <http://127.0.0.1:8000/> in your browser, and you'll see the default\\nhome page being displayed:\\n\\n![The MkDocs live server](img/screenshot.png)\\n\\nThe dev-server also supports auto-reloading, and will rebuild your documentation\\nwhenever anything in the configuration file, documentation directory, or theme\\ndirectory changes.\\n\\nOpen the `docs/index.md` document in your text editor of choice, change the\\ninitial heading to `MkLorum`, and save your changes. Your browser will\\nauto-reload and you should see your updated documentation immediately.\\n\\nNow try editing the configuration file: `mkdocs.yml`. Change the\\n[`site_name`][site_name] setting to `MkLorum` and save the file.\\n\\n```yaml\\nsite_name: MkLorum\\n```\"],\n",
              " 'uris': None,\n",
              " 'included': ['metadatas', 'documents', 'embeddings'],\n",
              " 'data': None,\n",
              " 'metadatas': [{'total_chunks': 11,\n",
              "   'source': 'mkdocs',\n",
              "   'chunk_size': 228,\n",
              "   'chunk_index': 0,\n",
              "   'file_path': 'getting-started.md'},\n",
              "  {'total_chunks': 11,\n",
              "   'chunk_index': 1,\n",
              "   'file_path': 'getting-started.md',\n",
              "   'chunk_size': 849,\n",
              "   'source': 'mkdocs'},\n",
              "  {'chunk_index': 2,\n",
              "   'total_chunks': 11,\n",
              "   'source': 'mkdocs',\n",
              "   'chunk_size': 989,\n",
              "   'file_path': 'getting-started.md'}]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify collection\n",
        "print(f\"Total documents in collection: {collection.count()}\")\n",
        "print(\"\\nSample documents:\")\n",
        "collection.peek(limit=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test Query\n",
        "\n",
        "Test the retrieval system with a sample question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DELIVERABLE 5: Sample Questions and Context from Vector DB\n",
            "================================================================================\n",
            "\n",
            "ğŸ“ Question: How do I install MkDocs?\n",
            "\n",
            "ğŸ“Š Retrieved 5 relevant chunks from vector database:\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Result 1:\n",
            "  Source: getting-started.md\n",
            "  Distance: 0.1027 (lower = more similar)\n",
            "  Content preview:\n",
            "  # Getting Started with MkDocs\n",
            "\n",
            "An introductory tutorial!\n",
            "\n",
            "---\n",
            "\n",
            "## Installation\n",
            "\n",
            "To install MkDocs, run the following command from the command line:\n",
            "\n",
            "```bash\n",
            "pip install mkdocs\n",
            "```\n",
            "\n",
            "For more details, see the [Installation Guide]....\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Result 2:\n",
            "  Source: user-guide\\installation.md\n",
            "  Distance: 0.1081 (lower = more similar)\n",
            "  Content preview:\n",
            "  ## Installing MkDocs\n",
            "\n",
            "Install the `mkdocs` package using pip:\n",
            "\n",
            "```bash\n",
            "pip install mkdocs\n",
            "```\n",
            "\n",
            "You should now have the `mkdocs` command installed on your system. Run `mkdocs\n",
            "--version` to check that everything worked okay.\n",
            "\n",
            "```console\n",
            "$ mkdocs --version\n",
            "mkdocs, version 1.2.0 from /usr/local/lib/pyth...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Result 3:\n",
            "  Source: dev-guide\\translations.md\n",
            "  Distance: 0.1400 (lower = more similar)\n",
            "  Content preview:\n",
            "  ### Fork and clone the MkDocs repository\n",
            "\n",
            "In the following steps you'll work with a fork of the MkDocs repository. Follow\n",
            "the instructions for [forking and cloning the MkDocs\n",
            "repository](../about/contributing.md#installing-for-development).\n",
            "\n",
            "To test the translations you also need to [install MkDocs ...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Result 4:\n",
            "  Source: user-guide\\README.md\n",
            "  Distance: 0.1407 (lower = more similar)\n",
            "  Content preview:\n",
            "  # User Guide\n",
            "\n",
            "Building Documentation with MkDocs\n",
            "\n",
            "---\n",
            "\n",
            "The MkDocs User Guide provides documentation for users of MkDocs. See\n",
            "[Getting Started] for an introductory tutorial. You can jump directly to a\n",
            "page listed below, or use the *next* and *previous* buttons in the navigation\n",
            "bar at the top of the ...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Result 5:\n",
            "  Source: getting-started.md\n",
            "  Distance: 0.1488 (lower = more similar)\n",
            "  Content preview:\n",
            "  ## Getting help\n",
            "\n",
            "See the [User Guide] for more complete documentation of all of MkDocs' features.\n",
            "\n",
            "To get help with MkDocs, please use the [GitHub discussions] or [GitHub issues].\n",
            "\n",
            "[Installation Guide]: user-guide/installation.md\n",
            "[docs_dir]: user-guide/configuration.md#docs_dir\n",
            "[deploy]: user-guide/...\n",
            "\n",
            "\n",
            "ğŸ’¡ You can test more questions by changing 'test_query' above\n",
            "   Or use the FastAPI app (app.py) to query interactively\n"
          ]
        }
      ],
      "source": [
        "test_query = \"How do I install MkDocs?\"\n",
        "results = collection.query(\n",
        "    query_texts=[test_query],\n",
        "    n_results=5,\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DELIVERABLE 5: Sample Questions and Context from Vector DB\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nğŸ“ Question: {test_query}\\n\")\n",
        "print(f\"ğŸ“Š Retrieved {len(results['documents'][0])} relevant chunks from vector database:\\n\")\n",
        "\n",
        "for i, (doc, metadata, distance) in enumerate(zip(\n",
        "    results['documents'][0], \n",
        "    results['metadatas'][0],\n",
        "    results.get('distances', [[]])[0] if 'distances' in results else [0] * len(results['documents'][0])\n",
        "), 1):\n",
        "    print(f\"{'â”€' * 80}\")\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"  Source: {metadata.get('file_path', 'Unknown')}\")\n",
        "    print(f\"  Distance: {distance:.4f} (lower = more similar)\")\n",
        "    print(f\"  Content preview:\")\n",
        "    print(f\"  {doc[:300]}...\")\n",
        "    print()\n",
        "\n",
        "print(\"\\nğŸ’¡ You can test more questions by changing 'test_query' above\")\n",
        "print(\"   Or use the FastAPI app (app.py) to query interactively\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
